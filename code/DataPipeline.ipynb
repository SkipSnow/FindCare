{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1><strong>1. Project overview</strong></h1>\n",
        "<ol type=\"a\">\n",
        "  <li>Documentation is found in the Project documentation folder.</li>\n",
        "  <li>This note-book is the data pipeline that enriches the provider collection in the Mongo db.</li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"border: none; height: 2px; background-color: blue; margin-top: 6px; margin-bottom: 0;\" />\n",
        "<small>Add all imports and get the DB</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "from Utilities.ChatHealthyMongoUtilities import ChatHealthyMongoUtilities\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import csv\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "from bson.decimal128 import Decimal128\n",
        "from pymongo import MongoClient\n",
        "\n",
        "\n",
        "load_dotenv()  # <-- REQUIRED for .env files\n",
        "\n",
        "conn_str = os.getenv(\"MONGO_connectionString\")\n",
        "if not conn_str:\n",
        "    raise EnvironmentError(\"MONGO_connectionString is not set\")\n",
        "\n",
        "DbUtil = ChatHealthyMongoUtilities(conn_str)\n",
        "DB = DbUtil.getConnection()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MongoDB connection successfully established and validated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get zip -to county csv and load it to the db but also get the FIPS county name file so the file can be enriched with the county name cross walk. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Results\n",
        "# -----------------------------\n",
        "@dataclass(frozen=True)\n",
        "class LoadResults:\n",
        "    # HUD source (rows from ZIP_COUNTY_*.csv)\n",
        "    hud_rows_read: int\n",
        "    hud_rows_skipped_missing_required: int  # missing ZIP/COUNTY/TOT_RATIO after parsing\n",
        "    hud_rows_skipped_non_county_fips: int   # COUNTY not present in county-level FIPS map (Summary Level != 050)\n",
        "    hud_rows_considered_for_normalization: int  # rows that passed validation and county-only filter\n",
        "    hud_rows_written: int                  # number of normalized ZIP docs written (unique ZIPs)\n",
        "\n",
        "    # County lookup source (Census all-geocodes)\n",
        "    county_lookup_rows_read: int\n",
        "    county_lookup_rows_written: int\n",
        "    county_lookup_counties_in_map: int     # how many county FIPS were captured (Summary Level 050)\n",
        "\n",
        "    # Raw HUD crosswalk (loaded AS-IS)\n",
        "    census_crosswalk_rows_read: int\n",
        "    census_crosswalk_rows_written: int\n",
        "\n",
        "    # Enrichment metric\n",
        "    normalized_zips_missing_county_name: int  # should be 0 when county-only filter is enforced\n",
        "\n",
        "    # Validation metrics\n",
        "    raw_hud_distinct_zips: int\n",
        "    normalized_distinct_zips: int\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def _is_collection_empty(col: Collection) -> bool:\n",
        "    return col.estimated_document_count() == 0\n",
        "\n",
        "\n",
        "def _s(v: Any) -> Optional[str]:\n",
        "    if v is None:\n",
        "        return None\n",
        "    s = str(v).strip()\n",
        "    return s if s != \"\" else None\n",
        "\n",
        "\n",
        "def _zip5(value: Any) -> Optional[str]:\n",
        "    if value is None:\n",
        "        return None\n",
        "    s = str(value).strip()\n",
        "    if s == \"\":\n",
        "        return None\n",
        "    digits = \"\".join(ch for ch in s if ch.isdigit())\n",
        "    if len(digits) == 0:\n",
        "        return None\n",
        "    if len(digits) > 5:\n",
        "        digits = digits[:5]\n",
        "    return digits.zfill(5)\n",
        "\n",
        "\n",
        "def _to_decimal128(value: Any) -> Optional[Decimal128]:\n",
        "    \"\"\"\n",
        "    Convert a numeric-looking value into BSON Decimal128.\n",
        "    Returns None if the value is blank/unparseable.\n",
        "    \"\"\"\n",
        "    if value is None:\n",
        "        return None\n",
        "    s = str(value).strip()\n",
        "    if s == \"\":\n",
        "        return None\n",
        "    try:\n",
        "        return Decimal128(Decimal(s))\n",
        "    except (InvalidOperation, ValueError):\n",
        "        return None\n",
        "\n",
        "\n",
        "def _read_csv_rows(path: str) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
        "    with open(path, \"r\", newline=\"\", encoding=\"utf-8-sig\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if not reader.fieldnames:\n",
        "            raise ValueError(f\"No header row found in CSV: {path}\")\n",
        "        headers = list(reader.fieldnames)\n",
        "        rows = [r for r in reader]\n",
        "        return headers, rows\n",
        "\n",
        "\n",
        "def _read_xlsx_rows(path: str) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
        "    from openpyxl import load_workbook  # type: ignore\n",
        "\n",
        "    wb = load_workbook(path, read_only=True, data_only=True)\n",
        "    ws = wb.active\n",
        "\n",
        "    rows_iter = ws.iter_rows(values_only=True)\n",
        "    try:\n",
        "        header_row = next(rows_iter)\n",
        "    except StopIteration:\n",
        "        raise ValueError(f\"Empty XLSX: {path}\")\n",
        "\n",
        "    headers = [str(h).strip() if h is not None else \"\" for h in header_row]\n",
        "    if not any(headers):\n",
        "        raise ValueError(f\"Header row appears empty in XLSX: {path}\")\n",
        "\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for row in rows_iter:\n",
        "        doc: Dict[str, Any] = {}\n",
        "        for i, h in enumerate(headers):\n",
        "            if h == \"\":\n",
        "                continue\n",
        "            doc[h] = row[i] if i < len(row) else None\n",
        "        if any(v is not None and str(v).strip() != \"\" for v in doc.values()):\n",
        "            out.append(doc)\n",
        "\n",
        "    return headers, out\n",
        "\n",
        "\n",
        "def _read_tabular_rows(path: str) -> Tuple[List[str], List[Dict[str, Any]]]:\n",
        "    ext = os.path.splitext(path.lower())[1]\n",
        "    if ext in [\".csv\", \".txt\"]:\n",
        "        return _read_csv_rows(path)\n",
        "    if ext in [\".xlsx\", \".xlsm\"]:\n",
        "        return _read_xlsx_rows(path)\n",
        "    raise ValueError(f\"Unsupported file type '{ext}'. Use .csv or .xlsx/.xlsm: {path}\")\n",
        "\n",
        "\n",
        "def _insert_many_in_batches(\n",
        "    col: Collection,\n",
        "    docs: List[Dict[str, Any]],\n",
        "    batch_size: int = 1000,\n",
        ") -> int:\n",
        "    if not docs:\n",
        "        return 0\n",
        "    total = 0\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        chunk = docs[i : i + batch_size]\n",
        "        col.insert_many(chunk, ordered=False)\n",
        "        total += len(chunk)\n",
        "    return total\n",
        "\n",
        "\n",
        "def _normalize_key(s: str) -> str:\n",
        "    return \"\".join(ch.lower() for ch in s if ch.isalnum())\n",
        "\n",
        "\n",
        "def _find_col(headers: List[str], candidates: List[str]) -> Optional[str]:\n",
        "    norm_map = {_normalize_key(h): h for h in headers if h is not None}\n",
        "    for c in candidates:\n",
        "        key = _normalize_key(c)\n",
        "        if key in norm_map:\n",
        "            return norm_map[key]\n",
        "    return None\n",
        "\n",
        "\n",
        "def _build_fips_to_county_name_map_from_rows(\n",
        "    headers: List[str],\n",
        "    rows: List[Dict[str, Any]],\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Builds map: 5-digit county FIPS -> county name.\n",
        "\n",
        "    Supports your \"all-geocodes-v2021.csv\" format:\n",
        "      - Summary Level\n",
        "      - State Code (FIPS)\n",
        "      - County Code (FIPS)\n",
        "      - Area Name (including legal/statistical area description)\n",
        "\n",
        "    County rows use Summary Level == '050'.\n",
        "    countyFips = zfill2(State Code) + zfill3(County Code)\n",
        "\n",
        "    Also includes fallback logic for other Census schemas.\n",
        "    \"\"\"\n",
        "    summary_col = _find_col(headers, [\"Summary Level\"])\n",
        "    state_code_col = _find_col(headers, [\"State Code (FIPS)\"])\n",
        "    county_code_col = _find_col(headers, [\"County Code (FIPS)\"])\n",
        "    area_name_col = _find_col(headers, [\"Area Name (including legal/statistical area description)\"])\n",
        "\n",
        "    is_all_geocodes = all([summary_col, state_code_col, county_code_col, area_name_col])\n",
        "\n",
        "    out: Dict[str, str] = {}\n",
        "\n",
        "    if is_all_geocodes:\n",
        "        for row in rows:\n",
        "            summary = str(row.get(summary_col, \"\")).strip()\n",
        "            if summary != \"050\":\n",
        "                continue\n",
        "\n",
        "            st_raw = row.get(state_code_col)\n",
        "            co_raw = row.get(county_code_col)\n",
        "            nm_raw = row.get(area_name_col)\n",
        "\n",
        "            st = \"\".join(ch for ch in str(st_raw).strip() if ch.isdigit()) if st_raw is not None else \"\"\n",
        "            co = \"\".join(ch for ch in str(co_raw).strip() if ch.isdigit()) if co_raw is not None else \"\"\n",
        "            name = str(nm_raw).strip() if nm_raw is not None else \"\"\n",
        "\n",
        "            if st == \"\" or co == \"\" or name == \"\":\n",
        "                continue\n",
        "\n",
        "            county_fips = st.zfill(2) + co.zfill(3)\n",
        "            out.setdefault(county_fips, name)\n",
        "\n",
        "        if not out:\n",
        "            raise ValueError(\n",
        "                \"Detected an all-geocodes style file, but found zero county rows (Summary Level '050'). \"\n",
        "                \"Inspect the 'Summary Level' column and tell me the value used for counties if it differs.\"\n",
        "            )\n",
        "        return out\n",
        "\n",
        "    # Fallback generic schema\n",
        "    statefp_col = _find_col(headers, [\"STATEFP\", \"STATEFP20\", \"STATEFP10\", \"StateFP\"])\n",
        "    countyfp_col = _find_col(headers, [\"COUNTYFP\", \"COUNTYFP20\", \"COUNTYFP10\", \"CountyFP\"])\n",
        "    fips_col = _find_col(headers, [\"FIPS\", \"GEOID\", \"COUNTYFIPS\", \"COUNTY_FIPS\", \"COUNTYFP\"])\n",
        "    name_col = _find_col(headers, [\"COUNTYNAME\", \"COUNTY_NAME\", \"NAMELSAD\", \"NAME\"])\n",
        "\n",
        "    if name_col is None:\n",
        "        raise ValueError(\n",
        "            \"Could not find a county name column in the county lookup file. \"\n",
        "            f\"Headers seen: {headers}\"\n",
        "        )\n",
        "\n",
        "    for row in rows:\n",
        "        nm_raw = row.get(name_col)\n",
        "        county_name = str(nm_raw).strip() if nm_raw is not None else \"\"\n",
        "        if county_name == \"\":\n",
        "            continue\n",
        "\n",
        "        fips: Optional[str] = None\n",
        "\n",
        "        if statefp_col and countyfp_col:\n",
        "            st = row.get(statefp_col)\n",
        "            co = row.get(countyfp_col)\n",
        "            if st is not None and co is not None:\n",
        "                st_s = \"\".join(ch for ch in str(st) if ch.isdigit()).zfill(2)\n",
        "                co_s = \"\".join(ch for ch in str(co) if ch.isdigit()).zfill(3)\n",
        "                if len(st_s) == 2 and len(co_s) == 3:\n",
        "                    fips = st_s + co_s\n",
        "\n",
        "        if fips is None and fips_col:\n",
        "            v = row.get(fips_col)\n",
        "            if v is not None:\n",
        "                digits = \"\".join(ch for ch in str(v) if ch.isdigit())\n",
        "                if len(digits) >= 5:\n",
        "                    fips = digits[-5:]\n",
        "\n",
        "        if fips:\n",
        "            out.setdefault(fips, county_name)\n",
        "\n",
        "    if not out:\n",
        "        raise ValueError(\n",
        "            \"Built an empty county FIPS -> county name map. Headers may not match expected patterns.\"\n",
        "        )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main loader\n",
        "# -----------------------------\n",
        "def load_county_zip_with_census_collections(\n",
        "    argDbConnection: MongoClient,\n",
        "    argDatabaseName: str,\n",
        "\n",
        "    # 2 source files\n",
        "    argHudCountyZipCsvPath: str,\n",
        "    argCountyFipsNameFilePath: str,\n",
        "\n",
        "    # 3 collections\n",
        "    argHudNormalizedCollectionName: str,\n",
        "    argCountyLookupCollectionName: str,\n",
        "    argCensusCrosswalkCollectionName: str,\n",
        "\n",
        "    argBatchSize: int = 1000,\n",
        ") -> LoadResults:\n",
        "    \"\"\"\n",
        "    Creates/loads THREE collections, using TWO source files:\n",
        "\n",
        "    Source file #1: argHudCountyZipCsvPath (HUD ZIP-County crosswalk CSV)\n",
        "      - Loaded AS-IS into argCensusCrosswalkCollectionName\n",
        "      - Also normalized into argHudNormalizedCollectionName (highest TOT_RATIO per ZIP)\n",
        "      - Normalized collection is restricted to COUNTY FIPS that exist in the county-only\n",
        "        lookup map (Summary Level 050).\n",
        "\n",
        "    Source file #2: argCountyFipsNameFilePath (County FIPS -> County Name reference)\n",
        "      - Loaded AS-IS into argCountyLookupCollectionName\n",
        "      - Used to enrich normalized records with countyName\n",
        "      - Supports Census \"all-geocodes\" CSV format\n",
        "\n",
        "    Normalized fields written:\n",
        "      ZIP                 -> zip (ZIP5 string)\n",
        "      COUNTY              -> countyFips (5-digit string)\n",
        "      (derived)           -> countyName (string, from Summary Level 050 rows only)\n",
        "      USPS_ZIP_PREF_CITY  -> mainCity\n",
        "      USPS_ZIP_PREF_STATE -> state\n",
        "      TOT_RATIO           -> percentOfZipInCounty (BSON Decimal128)\n",
        "\n",
        "    Loads each collection only if empty, and returns a detailed validation report.\n",
        "    \"\"\"\n",
        "    db = argDbConnection[argDatabaseName]\n",
        "\n",
        "    normalized_col = db[argHudNormalizedCollectionName]\n",
        "    county_lookup_col = db[argCountyLookupCollectionName]\n",
        "    census_crosswalk_col = db[argCensusCrosswalkCollectionName]\n",
        "\n",
        "    # Read HUD crosswalk once\n",
        "    hud_headers, hud_rows = _read_csv_rows(argHudCountyZipCsvPath)\n",
        "\n",
        "    required = [\"ZIP\", \"COUNTY\", \"USPS_ZIP_PREF_CITY\", \"USPS_ZIP_PREF_STATE\", \"TOT_RATIO\"]\n",
        "    missing = [h for h in required if h not in hud_headers]\n",
        "    if missing:\n",
        "        raise ValueError(\n",
        "            \"HUD CSV is missing required columns: \"\n",
        "            f\"{missing}. Headers seen: {hud_headers}\"\n",
        "        )\n",
        "\n",
        "    # Compute raw distinct zips in HUD (for validation)\n",
        "    raw_zip_set = set()\n",
        "    for row in hud_rows:\n",
        "        z = _zip5(row.get(\"ZIP\"))\n",
        "        if z:\n",
        "            raw_zip_set.add(z)\n",
        "    raw_hud_distinct_zips = len(raw_zip_set)\n",
        "\n",
        "    # 1) Load HUD crosswalk AS-IS into raw collection\n",
        "    census_crosswalk_rows_read = len(hud_rows)\n",
        "    census_crosswalk_rows_written = 0\n",
        "    if _is_collection_empty(census_crosswalk_col):\n",
        "        census_crosswalk_rows_written = _insert_many_in_batches(\n",
        "            census_crosswalk_col, hud_rows, batch_size=argBatchSize\n",
        "        )\n",
        "\n",
        "    # 2) Load county lookup AS-IS into lookup collection\n",
        "    county_headers, county_rows = _read_tabular_rows(argCountyFipsNameFilePath)\n",
        "\n",
        "    county_lookup_rows_read = len(county_rows)\n",
        "    county_lookup_rows_written = 0\n",
        "    if _is_collection_empty(county_lookup_col):\n",
        "        county_lookup_rows_written = _insert_many_in_batches(\n",
        "            county_lookup_col, county_rows, batch_size=argBatchSize\n",
        "        )\n",
        "\n",
        "    # Build county-only map (Summary Level 050)\n",
        "    fips_to_name = _build_fips_to_county_name_map_from_rows(county_headers, county_rows)\n",
        "    county_lookup_counties_in_map = len(fips_to_name)\n",
        "\n",
        "    # 3) Normalize HUD + enrich with countyName; enforce county-only via fips_to_name membership\n",
        "    hud_rows_read = 0\n",
        "    hud_rows_skipped_missing_required = 0\n",
        "    hud_rows_skipped_non_county_fips = 0\n",
        "    hud_rows_considered_for_normalization = 0\n",
        "\n",
        "    hud_rows_written = 0\n",
        "    normalized_zips_missing_county_name = 0\n",
        "\n",
        "    if _is_collection_empty(normalized_col):\n",
        "        best_by_zip: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "        for row in hud_rows:\n",
        "            hud_rows_read += 1\n",
        "\n",
        "            zip5 = _zip5(row.get(\"ZIP\"))\n",
        "            county_digits = \"\".join(ch for ch in str(row.get(\"COUNTY\", \"\")).strip() if ch.isdigit())\n",
        "            county_fips = county_digits.zfill(5) if county_digits else None\n",
        "            ratio128 = _to_decimal128(row.get(\"TOT_RATIO\"))\n",
        "\n",
        "            if zip5 is None or county_fips is None or ratio128 is None:\n",
        "                hud_rows_skipped_missing_required += 1\n",
        "                continue\n",
        "\n",
        "            # COUNTY-ONLY FILTER: Only accept counties present in the 050-only map\n",
        "            county_name = fips_to_name.get(county_fips)\n",
        "            if county_name is None:\n",
        "                hud_rows_skipped_non_county_fips += 1\n",
        "                continue\n",
        "\n",
        "            hud_rows_considered_for_normalization += 1\n",
        "\n",
        "            candidate = {\n",
        "                \"zip\": zip5,\n",
        "                \"countyFips\": county_fips,\n",
        "                \"countyName\": county_name,  # should always be present due to filter\n",
        "                \"mainCity\": _s(row.get(\"USPS_ZIP_PREF_CITY\")),\n",
        "                \"state\": _s(row.get(\"USPS_ZIP_PREF_STATE\")),\n",
        "                \"percentOfZipInCounty\": ratio128,  # Decimal128\n",
        "            }\n",
        "\n",
        "            current = best_by_zip.get(zip5)\n",
        "            if current is None:\n",
        "                best_by_zip[zip5] = candidate\n",
        "            else:\n",
        "                cur_val = current[\"percentOfZipInCounty\"].to_decimal()\n",
        "                new_val = ratio128.to_decimal()\n",
        "                if new_val > cur_val:\n",
        "                    best_by_zip[zip5] = candidate\n",
        "\n",
        "        docs = list(best_by_zip.values())\n",
        "\n",
        "        # Should be zero now, but keep metric anyway\n",
        "        normalized_zips_missing_county_name = sum(1 for d in docs if not d.get(\"countyName\"))\n",
        "\n",
        "        hud_rows_written = _insert_many_in_batches(\n",
        "            normalized_col, docs, batch_size=argBatchSize\n",
        "        )\n",
        "\n",
        "        # Helpful indexes for your common queries\n",
        "        normalized_col.create_index(\"zip\")\n",
        "        normalized_col.create_index([(\"zip\", 1), (\"countyFips\", 1)])\n",
        "        county_lookup_col.create_index(\"Summary Level\")\n",
        "        county_lookup_col.create_index(\"State Code (FIPS)\")\n",
        "        county_lookup_col.create_index(\"County Code (FIPS)\")\n",
        "        census_crosswalk_col.create_index(\"ZIP\")\n",
        "        census_crosswalk_col.create_index(\"COUNTY\")\n",
        "\n",
        "    else:\n",
        "        # If collection already exists, we still return validation stats from sources,\n",
        "        # but we will not rewrite normalized data.\n",
        "        hud_rows_read = len(hud_rows)\n",
        "        # We won't recompute these without reading DB; leave them as 0 to avoid false claims.\n",
        "        hud_rows_skipped_missing_required = 0\n",
        "        hud_rows_skipped_non_county_fips = 0\n",
        "        hud_rows_considered_for_normalization = 0\n",
        "        hud_rows_written = 0\n",
        "        normalized_zips_missing_county_name = 0\n",
        "\n",
        "    # Validation: distinct zips in normalized collection\n",
        "    normalized_distinct_zips = normalized_col.distinct(\"zip\")\n",
        "    normalized_distinct_zips_count = len(normalized_distinct_zips)\n",
        "\n",
        "    return LoadResults(\n",
        "        hud_rows_read=hud_rows_read,\n",
        "        hud_rows_skipped_missing_required=hud_rows_skipped_missing_required,\n",
        "        hud_rows_skipped_non_county_fips=hud_rows_skipped_non_county_fips,\n",
        "        hud_rows_considered_for_normalization=hud_rows_considered_for_normalization,\n",
        "        hud_rows_written=hud_rows_written,\n",
        "        county_lookup_rows_read=county_lookup_rows_read,\n",
        "        county_lookup_rows_written=county_lookup_rows_written,\n",
        "        county_lookup_counties_in_map=county_lookup_counties_in_map,\n",
        "        census_crosswalk_rows_read=census_crosswalk_rows_read,\n",
        "        census_crosswalk_rows_written=census_crosswalk_rows_written,\n",
        "        normalized_zips_missing_county_name=normalized_zips_missing_county_name,\n",
        "        raw_hud_distinct_zips=raw_hud_distinct_zips,\n",
        "        normalized_distinct_zips=normalized_distinct_zips_count,\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "zipCountyCSVPath=r\"C:\\chatHealthy\\Resources\\ZIP_COUNTY_092025.csv\"\n",
        "CountyFipsNameFilePath=r\"C:\\chatHealthy\\Resources\\all-geocodes-v2021.csv\"\n",
        "\n",
        "stats = results = load_county_zip_with_census_collections(\n",
        "    argDbConnection=DbUtil.getConnection(),\n",
        "    argDatabaseName=\"PublicHealthData\",\n",
        "    argHudCountyZipCsvPath=zipCountyCSVPath,\n",
        "    argCountyFipsNameFilePath=CountyFipsNameFilePath,\n",
        "    argHudNormalizedCollectionName=\"CountyZipNormalized\",\n",
        "    argCountyLookupCollectionName=\"CountyFipsLookup\",\n",
        "    argCensusCrosswalkCollectionName=\"HudZipCountyRaw\",\n",
        ")\n",
        "for key, value in stats.__dict__.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LoadResults(hud_rows_read=54574, hud_rows_skipped_missing_required=0, hud_rows_skipped_non_county_fips=0, hud_rows_considered_for_normalization=0, hud_rows_written=0, county_lookup_rows_read=43833, county_lookup_rows_written=0, county_lookup_counties_in_map=3221, census_crosswalk_rows_read=54574, census_crosswalk_rows_written=0, normalized_zips_missing_county_name=0, raw_hud_distinct_zips=39493, normalized_distinct_zips=39493)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}